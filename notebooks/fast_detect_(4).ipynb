{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "zzqfICPhuaug"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvx2xhxM54yG",
        "outputId": "f89220ca-e1b3-4d60-d4e7-b002030f2234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fast-detect-gpt'...\n",
            "remote: Enumerating objects: 762, done.\u001b[K\n",
            "remote: Counting objects: 100% (264/264), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 762 (delta 240), reused 209 (delta 209), pack-reused 498 (from 1)\u001b[K\n",
            "Receiving objects: 100% (762/762), 226.69 MiB | 17.94 MiB/s, done.\n",
            "Resolving deltas: 100% (574/574), done.\n",
            "Updating files: 100% (503/503), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/baoguangsheng/fast-detect-gpt.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd fast-detect-gpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aX6XEax_5-H9",
        "outputId": "058b61c2-fdda-4bd8-c102-ee7f09f72295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fast-detect-gpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy transformers datasets matplotlib tqdm openai nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJi2xJkx723e",
        "outputId": "07cb45dc-261c-4c28-ddc2-16c45e3f6300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scripts/local_infer.py\n",
        "# Copyright (c) Guangsheng Bao.\n",
        "#\n",
        "# This source code is licensed under the MIT license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import glob\n",
        "import argparse\n",
        "import json\n",
        "from model import load_tokenizer, load_model\n",
        "from fast_detect_gpt import get_sampling_discrepancy_analytic\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Considering balanced classification that p(D0) equals to p(D1), we have\n",
        "# p(D1|x) = p(x|D1) / (p(x|D1) + p(x|D0))\n",
        "def compute_prob_norm(x, mu0, sigma0, mu1, sigma1):\n",
        "    pdf_value0 = norm.pdf(x, loc=mu0, scale=sigma0)\n",
        "    pdf_value1 = norm.pdf(x, loc=mu1, scale=sigma1)\n",
        "    prob = pdf_value1 / (pdf_value0 + pdf_value1)\n",
        "    return prob\n",
        "\n",
        "class FastDetectGPT:\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.criterion_fn = get_sampling_discrepancy_analytic\n",
        "        self.scoring_tokenizer = load_tokenizer(args.scoring_model_name, args.cache_dir)\n",
        "        self.scoring_model = load_model(args.scoring_model_name, args.device, args.cache_dir)\n",
        "        self.scoring_model.eval()\n",
        "        if args.sampling_model_name != args.scoring_model_name:\n",
        "            self.sampling_tokenizer = load_tokenizer(args.sampling_model_name, args.cache_dir)\n",
        "            self.sampling_model = load_model(args.sampling_model_name, args.device, args.cache_dir)\n",
        "            self.sampling_model.eval()\n",
        "\n",
        "        distrib_params = {\n",
        "            'gpt-j-6B_gpt-neo-2.7B': {'mu0': 0.2713, 'sigma0': 0.9366, 'mu1': 2.2334, 'sigma1': 1.8731},\n",
        "            'gpt-neo-2.7B_gpt-neo-2.7B': {'mu0': -0.2489, 'sigma0': 0.9968, 'mu1': 1.8983, 'sigma1': 1.9935},\n",
        "            'falcon-7b_falcon-7b-instruct': {'mu0': -0.0707, 'sigma0': 0.9520, 'mu1': 2.9306, 'sigma1': 1.9039},\n",
        "        }\n",
        "        key = f'{args.sampling_model_name}_{args.scoring_model_name}'\n",
        "\n",
        "        # Fallback for KeyError\n",
        "        if key not in distrib_params:\n",
        "            print(f\"Warning: Key '{key}' not in distrib_params. Using 'gpt-neo-2.7B_gpt-neo-2.7B' as fallback.\")\n",
        "            key = 'gpt-neo-2.7B_gpt-neo-2.7B'\n",
        "\n",
        "        self.classifier = distrib_params[key]\n",
        "\n",
        "    # compute conditional probability curvature\n",
        "    def compute_crit(self, text):\n",
        "        tokenized = self.scoring_tokenizer(text, truncation=True, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to(self.args.device)\n",
        "        labels = tokenized.input_ids[:, 1:]\n",
        "        if labels.size(1) == 0: # Handle empty or single-token text\n",
        "            return float('nan'), 0\n",
        "        with torch.no_grad():\n",
        "            logits_score = self.scoring_model(**tokenized).logits[:, :-1]\n",
        "            if self.args.sampling_model_name == self.args.scoring_model_name:\n",
        "                logits_ref = logits_score\n",
        "            else:\n",
        "                tokenized = self.sampling_tokenizer(text, truncation=True, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to(self.args.device)\n",
        "                assert torch.all(tokenized.input_ids[:, 1:] == labels), \"Tokenizer is mismatch.\"\n",
        "                logits_ref = self.sampling_model(**tokenized).logits[:, :-1]\n",
        "            crit = self.criterion_fn(logits_ref, logits_score, labels)\n",
        "        return crit, labels.size(1)\n",
        "\n",
        "    # compute probability\n",
        "    def compute_prob(self, text):\n",
        "        crit, ntoken = self.compute_crit(text)\n",
        "        if np.isnan(crit):\n",
        "            return float('nan'), crit, ntoken\n",
        "        mu0 = self.classifier['mu0']\n",
        "        sigma0 = self.classifier['sigma0']\n",
        "        mu1 = self.classifier['mu1']\n",
        "        sigma1 = self.classifier['sigma1']\n",
        "        prob = compute_prob_norm(crit, mu0, sigma0, mu1, sigma1)\n",
        "        return prob, crit, ntoken\n",
        "\n",
        "# --- NEW FLEXIBLE MAIN BLOCK ---\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # --- Text argument is now OPTIONAL ---\n",
        "    parser.add_argument('--text', type=str, default=None, help='(Optional) Text to be analyzed.')\n",
        "    parser.add_argument('--sampling_model_name', type=str, default=\"gpt-neo-2.7B\")\n",
        "    parser.add_argument('--scoring_model_name', type=str, default=\"gpt-neo-2.7B\")\n",
        "    parser.add_argument('--device', type=str, default=\"cuda\")\n",
        "    parser.add_argument('--cache_dir', type=str, default=\"../cache\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # --- EDIT THIS VARIABLE TO TEST YOUR TEXT ---\n",
        "\n",
        "    default_text_to_analyze = \"\"\"\n",
        "    i am human\n",
        "\"\"\"\n",
        "\n",
        "    # Check if --text argument was provided\n",
        "    if args.text is not None:\n",
        "      text_to_analyze = args.text\n",
        "      print(\"Using text provided from command line.\")\n",
        "    else:\n",
        "      text_to_analyze = default_text_to_analyze\n",
        "      print(\"No --text argument found. Using default text from script.\")\n",
        "\n",
        "    # 1. Initialize the detector\n",
        "    print(\"Initializing detector...\")\n",
        "    detector = FastDetectGPT(args)\n",
        "    print(\"Detector initialized.\")\n",
        "\n",
        "    # 2. Estimate the probability\n",
        "    print(f\"\\nAnalyzing text: '{text_to_analyze.strip()[:100]}...'\")\n",
        "    prob, crit, ntokens = detector.compute_prob(text_to_analyze)\n",
        "\n",
        "    # 3. Print the result\n",
        "    print(f'\\n--- Result ---')\n",
        "    if np.isnan(crit):\n",
        "        print(f'Could not analyze text. It might be too short or invalid.')\n",
        "    else:\n",
        "        print(f'Fast-DetectGPT criterion is {crit:.4f}')\n",
        "        print(f'Probability of being machine-generated: {prob * 100:.0f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEZuFEWS_78w",
        "outputId": "578f6494-31ef-4bfb-8768-928a5ce0574a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/local_infer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/local_infer.py \\\n",
        "    --scoring_model_name falcon-7b-instruct \\\n",
        "    --sampling_model_name falcon-7b \\\n",
        "    --device cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2X9eIJS-IwS",
        "outputId": "c74845c5-8b9c-4884-d6ec-1db4b7484443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No --text argument found. Using default text from script.\n",
            "Initializing detector...\n",
            "tokenizer_config.json: 1.13kB [00:00, 6.20MB/s]\n",
            "tokenizer.json: 2.73MB [00:00, 104MB/s]\n",
            "special_tokens_map.json: 100% 281/281 [00:00<00:00, 2.10MB/s]\n",
            "Loading model tiiuae/falcon-7b-instruct...\n",
            "config.json: 1.05kB [00:00, 7.07MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "2025-10-24 01:42:20.869621: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-10-24 01:42:20.888578: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761270140.907682    1755 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761270140.913242    1755 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761270140.930607    1755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761270140.930636    1755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761270140.930639    1755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761270140.930642    1755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-24 01:42:20.935697: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors.index.json: 17.7kB [00:00, 73.3MB/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/9.95G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   0% 0.00/4.48G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 772k/9.95G [00:01<5:27:55, 506kB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 67.8M/9.95G [00:01<03:04, 53.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   0% 661k/4.48G [00:01<3:28:39, 358kB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   5% 202M/4.48G [00:02<00:30, 139MB/s]  \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 136M/9.95G [00:02<01:55, 85.2MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 539M/9.95G [00:02<00:20, 454MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   8% 337M/4.48G [00:02<00:20, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   9% 404M/4.48G [00:02<00:18, 221MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 740M/9.95G [00:02<00:17, 523MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 874M/9.95G [00:03<00:25, 351MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  12% 538M/4.48G [00:04<00:30, 127MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 1.01G/9.95G [00:04<00:46, 193MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  16% 739M/4.48G [00:05<00:29, 127MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.08G/9.95G [00:05<00:59, 149MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  18% 806M/4.48G [00:05<00:24, 148MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.14G/9.95G [00:05<00:50, 173MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  19% 873M/4.48G [00:06<00:20, 176MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.21G/9.95G [00:06<00:43, 203MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  21% 940M/4.48G [00:06<00:20, 172MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.28G/9.95G [00:06<00:54, 159MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  22% 1.01G/4.48G [00:07<00:30, 115MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.34G/9.95G [00:07<01:13, 117MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  24% 1.07G/4.48G [00:09<00:52, 65.0MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.41G/9.95G [00:09<02:05, 68.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  28% 1.28G/4.48G [00:10<00:24, 131MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.55G/9.95G [00:10<01:17, 109MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  30% 1.35G/4.48G [00:10<00:20, 153MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.68G/9.95G [00:11<01:18, 106MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  31% 1.41G/4.48G [00:11<00:30, 99.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.75G/9.95G [00:14<02:05, 65.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  33% 1.48G/4.48G [00:14<00:47, 62.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  36% 1.61G/4.48G [00:14<00:27, 103MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.95G/9.95G [00:14<01:08, 117MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  39% 1.75G/4.48G [00:14<00:19, 138MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 2.02G/9.95G [00:15<01:28, 89.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  40% 1.81G/4.48G [00:18<00:45, 58.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.15G/9.95G [00:18<01:45, 73.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.28G/9.95G [00:18<01:12, 106MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  43% 1.95G/4.48G [00:18<00:29, 85.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.48G/9.95G [00:19<00:51, 144MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  45% 2.02G/4.48G [00:19<00:32, 75.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.62G/9.95G [00:20<00:52, 141MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  46% 2.08G/4.48G [00:22<00:48, 49.5MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.68G/9.95G [00:22<01:32, 78.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  51% 2.28G/4.48G [00:22<00:23, 95.2MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.89G/9.95G [00:22<00:54, 129MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  52% 2.35G/4.48G [00:22<00:18, 113MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 2.95G/9.95G [00:23<00:54, 129MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 3.02G/9.95G [00:23<00:53, 130MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  54% 2.42G/4.48G [00:24<00:22, 91.4MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.09G/9.95G [00:26<01:31, 75.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  55% 2.49G/4.48G [00:26<00:31, 63.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.15G/9.95G [00:26<01:24, 80.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  57% 2.55G/4.48G [00:26<00:26, 73.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  60% 2.69G/4.48G [00:26<00:14, 121MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.29G/9.95G [00:27<00:52, 126MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  61% 2.75G/4.48G [00:27<00:12, 137MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.42G/9.95G [00:27<00:46, 140MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  63% 2.82G/4.48G [00:28<00:17, 94.2MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.49G/9.95G [00:28<00:59, 108MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  64% 2.89G/4.48G [00:29<00:18, 87.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.56G/9.95G [00:29<01:02, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.62G/9.95G [00:30<00:58, 107MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  66% 2.95G/4.48G [00:30<00:19, 79.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.69G/9.95G [00:30<00:59, 106MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.76G/9.95G [00:31<00:45, 135MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  67% 3.02G/4.48G [00:31<00:16, 90.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  69% 3.09G/4.48G [00:31<00:14, 98.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.82G/9.95G [00:32<01:04, 94.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  70% 3.16G/4.48G [00:32<00:14, 91.4MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.89G/9.95G [00:32<01:01, 98.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  72% 3.22G/4.48G [00:33<00:12, 97.3MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 3.96G/9.95G [00:33<00:58, 103MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  73% 3.29G/4.48G [00:33<00:12, 96.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 4.03G/9.95G [00:34<00:57, 102MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  75% 3.36G/4.48G [00:34<00:11, 95.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.10G/9.95G [00:35<01:05, 89.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  76% 3.42G/4.48G [00:35<00:11, 88.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  78% 3.49G/4.48G [00:36<00:10, 90.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.16G/9.95G [00:36<01:08, 85.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.23G/9.95G [00:36<01:01, 93.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  79% 3.56G/4.48G [00:36<00:10, 90.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.30G/9.95G [00:37<01:01, 91.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  81% 3.63G/4.48G [00:37<00:09, 91.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.36G/9.95G [00:37<00:56, 98.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  82% 3.69G/4.48G [00:38<00:08, 97.4MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.43G/9.95G [00:38<00:57, 96.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  84% 3.76G/4.48G [00:38<00:07, 95.4MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.50G/9.95G [00:40<01:20, 67.9MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  85% 3.83G/4.48G [00:40<00:09, 66.4MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.56G/9.95G [00:41<01:14, 72.1MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  87% 3.89G/4.48G [00:41<00:07, 74.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.70G/9.95G [00:41<00:41, 125MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  90% 4.03G/4.48G [00:41<00:03, 132MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.77G/9.95G [00:41<00:40, 127MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  93% 4.16G/4.48G [00:42<00:02, 147MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.83G/9.95G [00:43<01:09, 73.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  94% 4.23G/4.48G [00:45<00:04, 62.3MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.90G/9.95G [00:45<01:21, 61.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.10G/9.95G [00:45<00:38, 127MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.24G/9.95G [00:46<00:33, 141MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  97% 4.36G/4.48G [00:46<00:01, 76.4MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.30G/9.95G [00:46<00:36, 127MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  99% 4.43G/4.48G [00:46<00:00, 81.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.37G/9.95G [00:47<00:36, 127MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.44G/9.95G [00:47<00:35, 128MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.50G/9.95G [00:48<00:34, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.57G/9.95G [00:48<00:33, 132MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.64G/9.95G [00:49<00:31, 135MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.77G/9.95G [00:49<00:18, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.84G/9.95G [00:49<00:18, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.91G/9.95G [00:50<00:17, 232MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 5.97G/9.95G [00:50<00:20, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.04G/9.95G [00:50<00:20, 187MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.11G/9.95G [00:51<00:20, 186MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.17G/9.95G [00:51<00:20, 180MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.24G/9.95G [00:52<00:22, 162MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.31G/9.95G [00:52<00:23, 152MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.38G/9.95G [00:53<00:24, 147MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.44G/9.95G [00:53<00:21, 164MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.58G/9.95G [00:53<00:12, 260MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.64G/9.95G [00:54<00:16, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.71G/9.95G [00:54<00:15, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.78G/9.95G [00:54<00:15, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.84G/9.95G [00:55<00:16, 186MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.91G/9.95G [00:55<00:16, 187MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors: 100% 4.48G/4.48G [00:55<00:00, 80.4MB/s]\n",
            "\n",
            "model-00001-of-00002.safetensors:  70% 6.98G/9.95G [00:56<00:16, 178MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.05G/9.95G [00:56<00:18, 158MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.11G/9.95G [00:57<00:17, 160MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.18G/9.95G [00:57<00:16, 169MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.25G/9.95G [00:57<00:14, 182MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.31G/9.95G [00:58<00:16, 160MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.38G/9.95G [00:58<00:17, 150MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.45G/9.95G [00:59<00:17, 144MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.52G/9.95G [00:59<00:17, 140MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.58G/9.95G [01:00<00:14, 166MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.72G/9.95G [01:00<00:08, 278MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.78G/9.95G [01:00<00:07, 309MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.85G/9.95G [01:00<00:10, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.92G/9.95G [01:01<00:10, 195MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.98G/9.95G [01:01<00:10, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.05G/9.95G [01:01<00:09, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.12G/9.95G [01:02<00:10, 177MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.19G/9.95G [01:02<00:10, 161MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.25G/9.95G [01:03<00:11, 150MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.35G/9.95G [01:03<00:09, 178MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.41G/9.95G [01:04<00:07, 195MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.48G/9.95G [01:04<00:06, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.55G/9.95G [01:04<00:07, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.61G/9.95G [01:05<00:06, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.68G/9.95G [01:05<00:06, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.75G/9.95G [01:05<00:06, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.82G/9.95G [01:06<00:06, 188MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.88G/9.95G [01:06<00:06, 167MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 8.95G/9.95G [01:07<00:06, 154MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.02G/9.95G [01:07<00:05, 165MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.08G/9.95G [01:07<00:04, 185MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.15G/9.95G [01:07<00:03, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.22G/9.95G [01:08<00:03, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.28G/9.95G [01:08<00:02, 227MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.35G/9.95G [01:08<00:02, 273MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.42G/9.95G [01:08<00:01, 306MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.49G/9.95G [01:08<00:01, 328MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.55G/9.95G [01:09<00:01, 338MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.62G/9.95G [01:09<00:00, 357MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.69G/9.95G [01:09<00:00, 361MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.75G/9.95G [01:09<00:00, 369MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.82G/9.95G [01:09<00:00, 374MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.88G/9.95G [01:10<00:00, 371MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 9.95G/9.95G [01:10<00:00, 142MB/s]\n",
            "Fetching 2 files: 100% 2/2 [01:10<00:00, 35.22s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:03<00:00,  1.53s/it]\n",
            "generation_config.json: 100% 117/117 [00:00<00:00, 1.36MB/s]\n",
            "Moving model to GPU...DONE (3.90s)\n",
            "tokenizer_config.json: 100% 287/287 [00:00<00:00, 3.33MB/s]\n",
            "tokenizer.json: 2.73MB [00:00, 157MB/s]\n",
            "special_tokens_map.json: 100% 281/281 [00:00<00:00, 3.50MB/s]\n",
            "Loading model tiiuae/falcon-7b...\n",
            "config.json: 1.05kB [00:00, 7.33MB/s]\n",
            "model.safetensors.index.json: 17.7kB [00:00, 82.9MB/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/9.95G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   0% 0.00/4.48G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 1.14M/9.95G [00:01<4:13:19, 655kB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   0% 438k/4.48G [00:01<5:05:22, 245kB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   3% 146M/4.48G [00:01<00:40, 107MB/s]  \u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   6% 252M/4.48G [00:02<00:21, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   8% 371M/4.48G [00:02<00:16, 243MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  11% 476M/4.48G [00:02<00:12, 326MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 70.1M/9.95G [00:02<04:48, 34.3MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 539M/9.95G [00:02<00:29, 322MB/s]  \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  14% 610M/4.48G [00:02<00:11, 351MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  15% 677M/4.48G [00:03<00:12, 312MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  17% 759M/4.48G [00:03<00:16, 232MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 673M/9.95G [00:03<00:41, 224MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  18% 799M/4.48G [00:04<00:23, 156MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  19% 862M/4.48G [00:07<00:59, 60.4MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 807M/9.95G [00:07<01:46, 85.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  20% 889M/4.48G [00:07<01:05, 54.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 875M/9.95G [00:08<01:34, 95.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  21% 935M/4.48G [00:08<00:56, 62.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  22% 1.00G/4.48G [00:09<00:48, 71.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  25% 1.14G/4.48G [00:09<00:30, 111MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 942M/9.95G [00:09<01:57, 76.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.08G/9.95G [00:10<01:23, 106MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  27% 1.20G/4.48G [00:10<00:31, 105MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.14G/9.95G [00:10<01:15, 116MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.28G/9.95G [00:10<00:55, 157MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.41G/9.95G [00:11<00:43, 196MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  28% 1.24G/4.48G [00:11<00:39, 81.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.54G/9.95G [00:11<00:37, 226MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.61G/9.95G [00:12<00:47, 177MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.68G/9.95G [00:13<00:55, 148MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.75G/9.95G [00:13<00:55, 149MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  29% 1.31G/4.48G [00:13<00:59, 53.5MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.88G/9.95G [00:13<00:38, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.95G/9.95G [00:13<00:34, 233MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.08G/9.95G [00:14<00:23, 339MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  33% 1.49G/4.48G [00:14<00:30, 96.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.15G/9.95G [00:14<00:29, 264MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.21G/9.95G [00:14<00:31, 242MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  36% 1.63G/4.48G [00:15<00:27, 105MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.28G/9.95G [00:16<01:24, 90.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  38% 1.71G/4.48G [00:17<00:40, 69.3MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.42G/9.95G [00:17<01:06, 114MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.55G/9.95G [00:17<00:42, 173MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  39% 1.75G/4.48G [00:17<00:35, 76.5MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.62G/9.95G [00:19<01:15, 96.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  39% 1.77G/4.48G [00:20<01:06, 40.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.68G/9.95G [00:21<01:31, 79.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  41% 1.84G/4.48G [00:21<00:49, 53.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  42% 1.90G/4.48G [00:21<00:38, 66.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  43% 1.92G/4.48G [00:21<00:37, 67.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.75G/9.95G [00:21<01:26, 83.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  45% 2.00G/4.48G [00:23<00:39, 62.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  47% 2.11G/4.48G [00:23<00:25, 92.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.89G/9.95G [00:23<01:32, 76.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 2.95G/9.95G [00:24<01:19, 88.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.09G/9.95G [00:24<00:54, 127MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.22G/9.95G [00:24<00:43, 154MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  49% 2.18G/4.48G [00:25<00:33, 68.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  51% 2.27G/4.48G [00:25<00:25, 85.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.29G/9.95G [00:25<00:54, 121MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.42G/9.95G [00:26<00:40, 162MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  52% 2.33G/4.48G [00:26<00:25, 85.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.49G/9.95G [00:26<00:36, 176MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  53% 2.38G/4.48G [00:26<00:20, 101MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  54% 2.41G/4.48G [00:26<00:19, 105MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.56G/9.95G [00:27<00:44, 143MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.62G/9.95G [00:27<00:41, 152MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  56% 2.51G/4.48G [00:27<00:19, 104MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.69G/9.95G [00:28<00:38, 162MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.83G/9.95G [00:28<00:25, 242MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  57% 2.57G/4.48G [00:28<00:18, 107MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.89G/9.95G [00:28<00:26, 230MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 3.96G/9.95G [00:28<00:29, 206MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  58% 2.59G/4.48G [00:29<00:23, 80.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  59% 2.64G/4.48G [00:31<00:40, 45.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.09G/9.95G [00:32<01:12, 80.9MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  60% 2.67G/4.48G [00:32<00:46, 39.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  61% 2.74G/4.48G [00:34<00:42, 40.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.16G/9.95G [00:34<01:39, 58.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.29G/9.95G [00:34<01:03, 88.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  62% 2.77G/4.48G [00:34<00:36, 47.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  63% 2.82G/4.48G [00:36<00:42, 39.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  65% 2.93G/4.48G [00:36<00:24, 63.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.36G/9.95G [00:36<01:33, 59.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.49G/9.95G [00:37<01:06, 82.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  67% 2.99G/4.48G [00:37<00:22, 66.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  67% 3.01G/4.48G [00:37<00:20, 71.3MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.69G/9.95G [00:38<00:42, 125MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.76G/9.95G [00:38<00:39, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.90G/9.95G [00:39<00:30, 163MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  69% 3.10G/4.48G [00:39<00:20, 68.3MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.03G/9.95G [00:39<00:23, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.10G/9.95G [00:39<00:24, 196MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  71% 3.16G/4.48G [00:39<00:18, 71.5MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.16G/9.95G [00:40<00:23, 201MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  72% 3.21G/4.48G [00:40<00:14, 85.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  73% 3.26G/4.48G [00:40<00:12, 94.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.30G/9.95G [00:40<00:22, 205MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  75% 3.37G/4.48G [00:40<00:07, 143MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.37G/9.95G [00:41<00:23, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.43G/9.95G [00:41<00:22, 197MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  77% 3.44G/4.48G [00:41<00:08, 126MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.50G/9.95G [00:41<00:23, 192MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  77% 3.46G/4.48G [00:41<00:08, 118MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.63G/9.95G [00:42<00:20, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.70G/9.95G [00:43<00:33, 128MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  78% 3.49G/4.48G [00:43<00:19, 51.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  79% 3.55G/4.48G [00:44<00:13, 69.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.77G/9.95G [00:44<00:43, 96.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  80% 3.57G/4.48G [00:45<00:23, 39.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  80% 3.60G/4.48G [00:47<00:31, 28.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.84G/9.95G [00:48<01:32, 44.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.91G/9.95G [00:48<01:08, 58.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 5.97G/9.95G [00:49<00:52, 76.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  82% 3.67G/4.48G [00:49<00:22, 35.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.04G/9.95G [00:49<00:42, 92.1MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  82% 3.69G/4.48G [00:49<00:20, 39.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  83% 3.74G/4.48G [00:49<00:12, 59.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  85% 3.82G/4.48G [00:50<00:08, 81.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  87% 3.88G/4.48G [00:50<00:05, 101MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  88% 3.95G/4.48G [00:50<00:03, 136MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.11G/9.95G [00:51<00:59, 64.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.27G/9.95G [00:51<00:31, 116MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  89% 4.01G/4.48G [00:51<00:04, 100MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.33G/9.95G [00:51<00:27, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.40G/9.95G [00:51<00:21, 162MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  90% 4.04G/4.48G [00:51<00:04, 93.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  92% 4.11G/4.48G [00:52<00:02, 130MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.47G/9.95G [00:52<00:22, 153MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  93% 4.18G/4.48G [00:52<00:02, 143MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.53G/9.95G [00:52<00:21, 163MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.60G/9.95G [00:52<00:17, 194MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.67G/9.95G [00:53<00:14, 225MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  95% 4.24G/4.48G [00:53<00:01, 122MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.74G/9.95G [00:53<00:14, 216MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  96% 4.31G/4.48G [00:53<00:01, 138MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.80G/9.95G [00:53<00:15, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.87G/9.95G [00:53<00:12, 239MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  98% 4.38G/4.48G [00:54<00:00, 135MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.94G/9.95G [00:54<00:13, 223MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  99% 4.42G/4.48G [00:54<00:00, 138MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors: 100% 4.48G/4.48G [00:54<00:00, 81.6MB/s]\n",
            "\n",
            "model-00001-of-00002.safetensors:  70% 7.00G/9.95G [00:56<00:42, 69.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.07G/9.95G [00:57<00:36, 78.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.14G/9.95G [00:57<00:31, 88.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.21G/9.95G [00:58<00:27, 98.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.27G/9.95G [00:58<00:23, 114MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.34G/9.95G [00:58<00:18, 144MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.41G/9.95G [00:59<00:14, 175MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.47G/9.95G [00:59<00:11, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.54G/9.95G [00:59<00:10, 237MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.61G/9.95G [00:59<00:08, 264MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.67G/9.95G [00:59<00:07, 285MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.74G/9.95G [01:00<00:06, 335MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.81G/9.95G [01:00<00:06, 341MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.88G/9.95G [01:00<00:06, 340MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.94G/9.95G [01:00<00:05, 363MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 8.01G/9.95G [01:00<00:05, 362MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.08G/9.95G [01:00<00:05, 362MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.14G/9.95G [01:04<00:34, 52.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.21G/9.95G [01:12<01:19, 21.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.41G/9.95G [01:12<00:31, 49.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.61G/9.95G [01:12<00:15, 86.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.81G/9.95G [01:12<00:08, 136MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.02G/9.95G [01:12<00:04, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.22G/9.95G [01:12<00:02, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.41G/9.95G [01:12<00:01, 382MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.62G/9.95G [01:12<00:00, 502MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 9.95G/9.95G [01:13<00:00, 136MB/s]\n",
            "Fetching 2 files: 100% 2/2 [01:13<00:00, 36.71s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:03<00:00,  1.67s/it]\n",
            "generation_config.json: 100% 117/117 [00:00<00:00, 1.31MB/s]\n",
            "Moving model to GPU...DONE (3.71s)\n",
            "Detector initialized.\n",
            "\n",
            "Analyzing text: 'i am human...'\n",
            "\n",
            "--- Result ---\n",
            "Fast-DetectGPT criterion is -0.5977\n",
            "Probability of being machine-generated: 9%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Baseline"
      ],
      "metadata": {
        "id": "mC1dWDGst0Y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p baseline/exp_gpt3to4/results/"
      ],
      "metadata": {
        "id": "3Om1_QRh6VU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Fast-DetectGPT with GPT-Neo-2.7B (fits in Colab)\n",
        "# Using black-box setting: sampling and scoring with surrogate models\n",
        "\n",
        "dataset = \"xsum\"\n",
        "source_model = \"gpt-3.5-turbo\"  # The model that generated the text\n",
        "sampling_model = \"gpt-neo-2.7B\"  # Surrogate model for sampling\n",
        "scoring_model = \"gpt-neo-2.7B\"   # Surrogate model for scoring\n",
        "\n",
        "!python scripts/fast_detect_gpt.py \\\n",
        "    --sampling_model_name {sampling_model} \\\n",
        "    --scoring_model_name {scoring_model} \\\n",
        "    --dataset {dataset} \\\n",
        "    --dataset_file exp_gpt3to4/data/{dataset}_{source_model} \\\n",
        "    --output_file exp_gpt3to4/results/{dataset}_{source_model}.{sampling_model}_{scoring_model}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qikl5id99Uwn",
        "outputId": "df52ec3e-747b-4291-932d-efd0792866fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/fast-detect-gpt/scripts/fast_detect_gpt.py\", line 162, in <module>\n",
            "    experiment(args)\n",
            "  File \"/content/fast-detect-gpt/scripts/fast_detect_gpt.py\", line 74, in experiment\n",
            "    scoring_tokenizer = load_tokenizer(args.scoring_model_name, args.dataset, args.cache_dir)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: load_tokenizer() takes 2 positional arguments but 3 were given\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick Fix for Tokenizer Loading Issue\n",
        "import re\n",
        "import os\n",
        "\n",
        "def fix_tokenizer_calls():\n",
        "    \"\"\"Fix the load_tokenizer function calls\"\"\"\n",
        "\n",
        "    files_to_fix = [\n",
        "        'scripts/fast_detect_gpt.py',\n",
        "        'scripts/baselines.py',\n",
        "        'scripts/detect_gpt.py',\n",
        "        'scripts/detect_llm.py',\n",
        "    ]\n",
        "\n",
        "    for filepath in files_to_fix:\n",
        "        if not os.path.exists(filepath):\n",
        "            continue\n",
        "\n",
        "        with open(filepath, 'r') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        old_pattern = r'load_tokenizer\\(([^,]+),\\s*args\\.dataset,\\s*([^)]+)\\)'\n",
        "        if not re.search(old_pattern, content):\n",
        "            continue\n",
        "\n",
        "        # Create backup\n",
        "        with open(filepath + '.backup', 'w') as f:\n",
        "            f.write(content)\n",
        "\n",
        "        # Apply fix\n",
        "        fixed_content = re.sub(\n",
        "            r'load_tokenizer\\(([^,]+),\\s*args\\.dataset,\\s*([^)]+)\\)',\n",
        "            r'load_tokenizer(\\1, \\2)',\n",
        "            content\n",
        "        )\n",
        "\n",
        "        with open(filepath, 'w') as f:\n",
        "            f.write(fixed_content)\n",
        "\n",
        "        print(f\" Fixed: {filepath}\")\n",
        "\n",
        "    print(\"\\n Fix applied!\")\n",
        "\n",
        "fix_tokenizer_calls()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EyhZdD19tUX",
        "outputId": "c592ca14-9ecf-406c-ba29-dc7efacc3d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fixed: scripts/fast_detect_gpt.py\n",
            "\n",
            " Fix applied!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/fast_detect_gpt.py \\\n",
        "    --sampling_model_name {sampling_model} \\\n",
        "    --scoring_model_name {scoring_model} \\\n",
        "    --dataset {dataset} \\\n",
        "    --dataset_file exp_gpt3to4/data/{dataset}_{source_model} \\\n",
        "    --output_file baseline/exp_gpt3to4/results/{dataset}_{source_model}.{sampling_model}_{scoring_model}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPAJUBXF-BQb",
        "outputId": "977b7892-b697-4240-98ec-f9e2a7d4fed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rtokenizer_config.json:   0% 0.00/200 [00:00<?, ?B/s]\rtokenizer_config.json: 100% 200/200 [00:00<00:00, 1.25MB/s]\n",
            "config.json: 1.46kB [00:00, 6.89MB/s]\n",
            "vocab.json: 798kB [00:00, 55.9MB/s]\n",
            "merges.txt: 456kB [00:00, 124MB/s]\n",
            "special_tokens_map.json: 100% 90.0/90.0 [00:00<00:00, 589kB/s]\n",
            "Loading model EleutherAI/gpt-neo-2.7B...\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "2025-10-24 01:45:25.066775: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-10-24 01:45:25.083229: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761270325.104219    2779 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761270325.110591    2779 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761270325.126606    2779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761270325.126631    2779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761270325.126634    2779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761270325.126637    2779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-24 01:45:25.131543: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 10.7G/10.7G [00:49<00:00, 214MB/s]\n",
            "Moving model to GPU...DONE (1.56s)\n",
            "Raw data loaded from exp_gpt3to4/data/xsum_gpt-3.5-turbo.raw_data.json\n",
            "Computing sampling_discrepancy criterion: 100% 150/150 [00:11<00:00, 12.65it/s]\n",
            "Real mean/std: -0.09/1.03, Samples mean/std: 3.02/0.80\n",
            "Criterion sampling_discrepancy_threshold ROC AUC: 0.9920, PR AUC: 0.9930\n",
            "Results written into baseline/exp_gpt3to4/results/xsum_gpt-3.5-turbo.gpt-neo-2.7B_gpt-neo-2.7B.sampling_discrepancy.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## performance summarization function"
      ],
      "metadata": {
        "id": "BE8H5ib12dld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def summarize_performance(json_file_path):\n",
        "    \"\"\"\n",
        "    Loads a JSON experiment file from fast-detect-gpt, calculates performance\n",
        "    statistics, and prints a formatted summary.\n",
        "\n",
        "    Args:\n",
        "        json_file_path (str): The file path to the experiment's JSON output file.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if the file exists\n",
        "    if not os.path.exists(json_file_path):\n",
        "        print(f\"Error: The file '{json_file_path}' was not found.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Open and load the JSON data\n",
        "        with open(json_file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # --- Extract Metadata ---\n",
        "        test_name = data.get('name', 'N/A')\n",
        "        n_samples = data.get('info', {}).get('n_samples', 'N/A')\n",
        "\n",
        "        # --- Extract Final Metrics ---\n",
        "        roc_auc = data.get('metrics', {}).get('roc_auc', 'N/A')\n",
        "        pr_auc = data.get('pr_metrics', {}).get('pr_auc', 'N/A')\n",
        "\n",
        "        # --- Extract and Calculate Prediction Statistics ---\n",
        "        predictions = data.get('predictions', {})\n",
        "        real_scores = predictions.get('real', [])\n",
        "        sampled_scores = predictions.get('samples', [])\n",
        "\n",
        "        # Calculate mean and std dev for human (\"real\") scores\n",
        "        if real_scores:\n",
        "            real_mean = np.mean(real_scores)\n",
        "            real_std = np.std(real_scores)\n",
        "        else:\n",
        "            real_mean = 'N/A'\n",
        "            real_std = 'N/A'\n",
        "\n",
        "        # Calculate mean and std dev for machine (\"sampled\") scores\n",
        "        if sampled_scores:\n",
        "            sampled_mean = np.mean(sampled_scores)\n",
        "            sampled_std = np.std(sampled_scores)\n",
        "        else:\n",
        "            sampled_mean = 'N/A'\n",
        "            sampled_std = 'N/A'\n",
        "\n",
        "        # --- Print the Summary Report ---\n",
        "        print(f\"--- Performance Summary for: {os.path.basename(json_file_path)} ---\")\n",
        "        print(f\"Test Name: {test_name}\")\n",
        "        print(f\"Number of Samples: {n_samples}\")\n",
        "        print(\"\\n--- Prediction Scores (Criterion) ---\")\n",
        "\n",
        "        # Print real scores statistics\n",
        "        if isinstance(real_mean, float):\n",
        "            print(f\"Human (Real) Scores Mean: {real_mean:.4f}\")\n",
        "            print(f\"Human (Real) Scores Std Dev: {real_std:.4f}\")\n",
        "        else:\n",
        "            print(\"Human (Real) Scores Mean: N/A\")\n",
        "            print(\"Human (Real) Scores Std Dev: N/A\")\n",
        "\n",
        "        # Print sampled scores statistics\n",
        "        if isinstance(sampled_mean, float):\n",
        "            print(f\"Machine (Sampled) Scores Mean: {sampled_mean:.4f}\")\n",
        "            print(f\"Machine (Sampled) Scores Std Dev: {sampled_std:.4f}\")\n",
        "        else:\n",
        "            print(\"Machine (Sampled) Scores Mean: N/A\")\n",
        "            print(\"Machine (Sampled) Scores Std Dev: N/A\")\n",
        "\n",
        "        print(\"\\n--- Key Metrics ---\")\n",
        "\n",
        "        # Print final metrics\n",
        "        if isinstance(roc_auc, float):\n",
        "            print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "        else:\n",
        "            print(\"ROC AUC: N/A\")\n",
        "\n",
        "        if isinstance(pr_auc, float):\n",
        "            print(f\"PR AUC: {pr_auc:.4f}\")\n",
        "        else:\n",
        "            print(\"PR AUC: N/A\")\n",
        "\n",
        "        print(\"-------------------------------------------------\" + \"-\" * len(os.path.basename(json_file_path)))\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Could not decode JSON from the file '{json_file_path}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "eZO28J3OCFhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = './baseline/exp_gpt3to4/results/xsum_gpt-3.5-turbo.gpt-neo-2.7B_gpt-neo-2.7B.sampling_discrepancy.json'\n",
        "\n",
        "summarize_performance(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rl4W8v_EFWrx",
        "outputId": "304f2d09-739c-4023-bafe-4ed750c175a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Performance Summary for: xsum_gpt-3.5-turbo.gpt-neo-2.7B_gpt-neo-2.7B.sampling_discrepancy.json ---\n",
            "Test Name: sampling_discrepancy_threshold\n",
            "Number of Samples: 150\n",
            "\n",
            "--- Prediction Scores (Criterion) ---\n",
            "Human (Real) Scores Mean: -0.0929\n",
            "Human (Real) Scores Std Dev: 1.0344\n",
            "Machine (Sampled) Scores Mean: 3.0247\n",
            "Machine (Sampled) Scores Std Dev: 0.7999\n",
            "\n",
            "--- Key Metrics ---\n",
            "ROC AUC: 0.9920\n",
            "PR AUC: 0.9930\n",
            "-----------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Synonym Attack"
      ],
      "metadata": {
        "id": "af2lVl_d0FLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# Download required NLTK data (updated for newer NLTK versions)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # Added for newer NLTK\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')  # Added for newer NLTK\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    \"\"\"Convert Penn Treebank POS to WordNet POS\"\"\"\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "def get_synonyms(word, pos):\n",
        "    \"\"\"Get WordNet synonyms for a word\"\"\"\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word, pos=pos):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonym = lemma.name().replace('_', ' ')\n",
        "            if synonym.lower() != word.lower():\n",
        "                synonyms.add(synonym)\n",
        "    return list(synonyms)\n",
        "\n",
        "def synonym_attack(text, replacement_rate=0.1):\n",
        "    \"\"\"Replace 10% of tokens with synonyms\"\"\"\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    # Target content words (nouns, verbs, adjectives)\n",
        "    content_word_indices = [\n",
        "        i for i, (word, pos) in enumerate(pos_tags)\n",
        "        if pos.startswith(('NN', 'VB', 'JJ')) and word.isalpha()\n",
        "    ]\n",
        "\n",
        "    # Calculate exact number to replace based on total tokens\n",
        "    num_to_replace = int(len(tokens) * replacement_rate)\n",
        "\n",
        "    if not content_word_indices:\n",
        "        return text, 0\n",
        "\n",
        "    # Select random content words to replace\n",
        "    indices_to_replace = random.sample(\n",
        "        content_word_indices,\n",
        "        min(num_to_replace, len(content_word_indices))\n",
        "    )\n",
        "\n",
        "    modified_tokens = tokens.copy()\n",
        "    replacements_made = 0\n",
        "\n",
        "    for idx in indices_to_replace:\n",
        "        word, pos = pos_tags[idx]\n",
        "        wn_pos = get_wordnet_pos(pos)\n",
        "        synonyms = get_synonyms(word, wn_pos)\n",
        "\n",
        "        if synonyms:\n",
        "            modified_tokens[idx] = random.choice(synonyms)\n",
        "            replacements_made += 1\n",
        "\n",
        "    # Reconstruct text (simple join - may need refinement for punctuation)\n",
        "    result = ' '.join(modified_tokens)\n",
        "    # Fix common spacing issues\n",
        "    result = result.replace(' .', '.').replace(' ,', ',').replace(' !', '!').replace(' ?', '?')\n",
        "\n",
        "    return result, replacements_made\n",
        "\n",
        "def strategic_perturbation_attack(text, replacement_rate=0.1):\n",
        "    \"\"\"\n",
        "    Replace common function words and determiners\n",
        "    These often don't change meaning much but affect probability\n",
        "    \"\"\"\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Substitution dictionary\n",
        "    substitutions = {\n",
        "        'the': ['a', 'this', 'that'],\n",
        "        'a': ['the', 'one'],\n",
        "        'an': ['a', 'the'],\n",
        "        'is': ['was', 'seems', 'appears'],\n",
        "        'are': ['were', 'seem'],\n",
        "        'was': ['is', 'seemed'],\n",
        "        'were': ['are', 'seemed'],\n",
        "        'very': ['quite', 'really', 'extremely'],\n",
        "        'said': ['stated', 'mentioned', 'noted'],\n",
        "        'also': ['additionally', 'furthermore', 'moreover'],\n",
        "        'however': ['but', 'nevertheless', 'yet'],\n",
        "        'therefore': ['thus', 'hence', 'consequently'],\n",
        "    }\n",
        "\n",
        "    num_to_replace = int(len(tokens) * replacement_rate)\n",
        "    modified_tokens = tokens.copy()\n",
        "    replacements = 0\n",
        "\n",
        "    # Randomly shuffle indices to replace\n",
        "    replaceable_indices = [\n",
        "        i for i, token in enumerate(tokens)\n",
        "        if token.lower() in substitutions\n",
        "    ]\n",
        "    random.shuffle(replaceable_indices)\n",
        "\n",
        "    for idx in replaceable_indices[:num_to_replace]:\n",
        "        token = tokens[idx]\n",
        "        token_lower = token.lower()\n",
        "\n",
        "        if token_lower in substitutions:\n",
        "            replacement = random.choice(substitutions[token_lower])\n",
        "            # Preserve capitalization\n",
        "            if token[0].isupper():\n",
        "                replacement = replacement.capitalize()\n",
        "            modified_tokens[idx] = replacement\n",
        "            replacements += 1\n",
        "\n",
        "    result = ' '.join(modified_tokens)\n",
        "    result = result.replace(' .', '.').replace(' ,', ',').replace(' !', '!').replace(' ?', '?')\n",
        "\n",
        "    return result, replacements\n",
        "\n",
        "def apply_attack_to_dataset(\n",
        "    input_file,\n",
        "    output_file,\n",
        "    attack_function,\n",
        "    replacement_rate=0.1\n",
        "):\n",
        "    \"\"\"\n",
        "    Apply attack to Fast-DetectGPT dataset (raw_data.json format)\n",
        "    \"\"\"\n",
        "    # Load dataset\n",
        "    with open(input_file, 'r') as f:\n",
        "        dataset = json.load(f)\n",
        "\n",
        "    attacked_dataset = {\n",
        "        'sampled': [],\n",
        "        'original': dataset.get('original', [])\n",
        "    }\n",
        "\n",
        "    stats = {\n",
        "        'total_samples': 0,\n",
        "        'total_tokens_original': 0,\n",
        "        'total_tokens_replaced': 0,\n",
        "        'successful_attacks': 0\n",
        "    }\n",
        "\n",
        "    for original_text in dataset['sampled']:\n",
        "        # Apply attack\n",
        "        attacked_text, num_replaced = attack_function(\n",
        "            original_text,\n",
        "            replacement_rate\n",
        "        )\n",
        "\n",
        "        attacked_dataset['sampled'].append(attacked_text)\n",
        "\n",
        "        # Track statistics\n",
        "        original_tokens = len(nltk.word_tokenize(original_text))\n",
        "        stats['total_samples'] += 1\n",
        "        stats['total_tokens_original'] += original_tokens\n",
        "        stats['total_tokens_replaced'] += num_replaced\n",
        "        if num_replaced > 0:\n",
        "            stats['successful_attacks'] += 1\n",
        "\n",
        "    # Save attacked dataset\n",
        "    output_path = Path(output_file)\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(attacked_dataset, f, indent=2)\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\" Attacked dataset saved to {output_file}\")\n",
        "    print(f\"\\n=== Attack Statistics ===\")\n",
        "    print(f\"Total samples processed: {stats['total_samples']}\")\n",
        "    print(f\"Successful attacks: {stats['successful_attacks']}\")\n",
        "    print(f\"Total original tokens: {stats['total_tokens_original']}\")\n",
        "    print(f\"Total tokens replaced: {stats['total_tokens_replaced']}\")\n",
        "    print(f\"Actual replacement rate: {stats['total_tokens_replaced']/stats['total_tokens_original']:.2%}\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "# ===========================================\n",
        "# MAIN EXPERIMENT\n",
        "# ===========================================\n",
        "\n",
        "dataset = \"xsum\"\n",
        "source_model = \"gpt-3.5-turbo\"\n",
        "\n",
        "# Define paths (update base path to your environment)\n",
        "base_path = \"exp_gpt3to4/data\"\n",
        "input_file = f\"{base_path}/{dataset}_{source_model}.raw_data.json\"\n",
        "\n",
        "# Test different attack strategies\n",
        "attack_strategies = [\n",
        "    (\"synonym\", synonym_attack),\n",
        "    (\"strategic\", strategic_perturbation_attack),\n",
        "]\n",
        "\n",
        "print(\"Starting adversarial attack experiment...\\n\")\n",
        "\n",
        "for attack_name, attack_func in attack_strategies:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Running {attack_name.upper()} attack\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    output_file = f\"{base_path}/{dataset}_{source_model}_attacked_{attack_name}_10pct.raw_data.json\"\n",
        "\n",
        "    stats = apply_attack_to_dataset(\n",
        "        input_file,\n",
        "        output_file,\n",
        "        attack_func,\n",
        "        replacement_rate=0.1\n",
        "    )\n",
        "\n",
        "    print(f\"\\nOutput saved: {output_file}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"All attacks completed!\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCvBk6ZR0Gcg",
        "outputId": "fa5566ae-dd18-4b43-9f63-f8473a3841e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting adversarial attack experiment...\n",
            "\n",
            "\n",
            "==================================================\n",
            "Running SYNONYM attack\n",
            "==================================================\n",
            " Attacked dataset saved to exp_gpt3to4/data/xsum_gpt-3.5-turbo_attacked_synonym_10pct.raw_data.json\n",
            "\n",
            "=== Attack Statistics ===\n",
            "Total samples processed: 150\n",
            "Successful attacks: 150\n",
            "Total original tokens: 30391\n",
            "Total tokens replaced: 2563\n",
            "Actual replacement rate: 8.43%\n",
            "\n",
            "Output saved: exp_gpt3to4/data/xsum_gpt-3.5-turbo_attacked_synonym_10pct.raw_data.json\n",
            "\n",
            "==================================================\n",
            "Running STRATEGIC attack\n",
            "==================================================\n",
            " Attacked dataset saved to exp_gpt3to4/data/xsum_gpt-3.5-turbo_attacked_strategic_10pct.raw_data.json\n",
            "\n",
            "=== Attack Statistics ===\n",
            "Total samples processed: 150\n",
            "Successful attacks: 150\n",
            "Total original tokens: 30391\n",
            "Total tokens replaced: 2882\n",
            "Actual replacement rate: 9.48%\n",
            "\n",
            "Output saved: exp_gpt3to4/data/xsum_gpt-3.5-turbo_attacked_strategic_10pct.raw_data.json\n",
            "\n",
            "==================================================\n",
            "All attacks completed!\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = \"xsum\"\n",
        "source_model = \"gpt-3.5-turbo\"\n",
        "sampling_model = \"gpt-neo-2.7B\"  # Surrogate model for sampling\n",
        "scoring_model = \"gpt-neo-2.7B\"   # Surrogate model for scoring\n",
        "\n",
        "\n",
        "print(\"\\n=== Running synonym attack evaluation ===\")\n",
        "!python scripts/fast_detect_gpt.py \\\n",
        "    --sampling_model_name {sampling_model} \\\n",
        "    --scoring_model_name {scoring_model} \\\n",
        "    --dataset {dataset} \\\n",
        "    --dataset_file exp_gpt3to4/data/{dataset}_{source_model}_attacked_synonym_10pct \\\n",
        "    --output_file exp_gpt3to4/results/{dataset}_{source_model}_attacked_synonym\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUE8y7cU17YU",
        "outputId": "0c0a877a-5f1c-4c1e-9d68-88d947ae21d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Running synonym attack evaluation ===\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\", line 407, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/requests/models.py\", line 1026, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/falcon-7b_instruct/resolve/main/tokenizer_config.json\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 479, in cached_files\n",
            "    hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1010, in hf_hub_download\n",
            "    return _hf_hub_download_to_cache_dir(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1117, in _hf_hub_download_to_cache_dir\n",
            "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1658, in _raise_on_head_call_error\n",
            "    raise head_call_error\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1546, in _get_metadata_or_catch_error\n",
            "    metadata = get_hf_file_metadata(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1463, in get_hf_file_metadata\n",
            "    r = _request_wrapper(\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 286, in _request_wrapper\n",
            "    response = _request_wrapper(\n",
            "               ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 310, in _request_wrapper\n",
            "    hf_raise_for_status(response)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\", line 457, in hf_raise_for_status\n",
            "    raise _format(RepositoryNotFoundError, message, response) from e\n",
            "huggingface_hub.errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-68fada89-064ba0176f644259186e644b;3482613e-d0e7-49df-8457-ad61b53b3e74)\n",
            "\n",
            "Repository Not Found for url: https://huggingface.co/falcon-7b_instruct/resolve/main/tokenizer_config.json.\n",
            "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
            "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
            "Invalid username or password.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/fast-detect-gpt/scripts/fast_detect_gpt.py\", line 162, in <module>\n",
            "    experiment(args)\n",
            "  File \"/content/fast-detect-gpt/scripts/fast_detect_gpt.py\", line 74, in experiment\n",
            "    scoring_tokenizer = load_tokenizer(args.scoring_model_name, args.cache_dir)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/fast-detect-gpt/scripts/model.py\", line 63, in load_tokenizer\n",
            "    base_tokenizer = from_pretrained(AutoTokenizer, model_fullname, optional_tok_kwargs, cache_dir=cache_dir)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/fast-detect-gpt/scripts/model.py\", line 16, in from_pretrained\n",
            "    return cls.from_pretrained(model_name, **kwargs, cache_dir=cache_dir)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\", line 1073, in from_pretrained\n",
            "    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\", line 905, in get_tokenizer_config\n",
            "    resolved_config_file = cached_file(\n",
            "                           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 322, in cached_file\n",
            "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 511, in cached_files\n",
            "    raise OSError(\n",
            "OSError: falcon-7b_instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_performance('/content/fast-detect-gpt/exp_gpt3to4/results/xsum_gpt-3.5-turbo_attacked_synonym.sampling_discrepancy.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPZb5Qu62kew",
        "outputId": "61852d2d-a20d-4d1f-a3cb-9149417af06c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The file '/content/fast-detect-gpt/exp_gpt3to4/results/xsum_gpt-3.5-turbo_attacked_synonym.sampling_discrepancy.json' was not found.\n"
          ]
        }
      ]
    }
  ]
}